{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wienier filter training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T13:31:37.331268Z",
     "start_time": "2020-05-05T13:31:37.329220Z"
    }
   },
   "outputs": [],
   "source": [
    "import loading\n",
    "import spir\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T13:31:39.939285Z",
     "start_time": "2020-05-05T13:31:39.908716Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save.p contains a dictionary with keys == filenames. Each key contains a list\n",
    "# of interferences\n",
    "with open('save.p', 'rb') as f:\n",
    "    inter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate interference cov matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T14:52:26.048125Z",
     "start_time": "2020-04-28T12:40:32.825297Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate covariance matrix for each interference\n",
    "\n",
    "lag = 50\n",
    "\n",
    "for filename in inter:\n",
    "    if len(inter[filename]):\n",
    "        print(filename)\n",
    "        (fs, data, labels) = loading.loadRecording(filename + '.edf')\n",
    "        folder = '/'.join(filename.split('/')[6:-1])\n",
    "        folder = os.path.join('results-ii', folder)\n",
    "        file = filename.split('/')[-1]\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "        for j in range(len(inter[filename])):\n",
    "            rnn = spir.build_cov(data, [[inter[filename][j][0]/fs, inter[filename][j][1]/fs]], lag, fs)\n",
    "            np.save(os.path.join(folder, file +  '-rii-{}'.format(j)), rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T13:32:18.434801Z",
     "start_time": "2020-05-05T13:31:47.157186Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load previously calculated covariance matrices\n",
    "\n",
    "rnns = list()\n",
    "for filename in inter:\n",
    "    if len(inter[filename]):\n",
    "        folder = '/'.join(filename.split('/')[6:-1])\n",
    "        folder = os.path.join('results-ii', folder)\n",
    "        file = filename.split('/')[-1]\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "        for j in range(len(inter[filename])):\n",
    "            rnn = np.load(os.path.join(folder, file +  '-rii-{}.npy'.format(j)))\n",
    "            rnns.append(rnn.flatten())\n",
    "rnns = np.array(rnns)\n",
    "\n",
    "# Bad element == 70\n",
    "rnns = np.delete(rnns, 70, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress Rnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T13:35:23.194351Z",
     "start_time": "2020-05-05T13:32:22.952159Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.999)\n",
    "pca.fit(rnns)\n",
    "compressed = pca.fit_transform(rnns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:17:20.726270Z",
     "start_time": "2020-04-29T09:17:20.719411Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of compressed components: {}'.format(compressed.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "with h5py.File('compressed.h5', 'r') as h5f:\n",
    "    compressed = np.array(h5f['compressed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:16:03.111757Z",
     "start_time": "2020-04-29T09:16:00.352273Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Find n-clusters\n",
    "def calculate_WSS(points, kmax):\n",
    "    sse = []\n",
    "    for k in range(1, kmax+1):\n",
    "        kmeans = KMeans(n_clusters = k).fit(points)\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        pred_clusters = kmeans.predict(points)\n",
    "        curr_sse = 0\n",
    "\n",
    "        for i in range(len(points)):\n",
    "            curr_center = centroids[pred_clusters[i]]\n",
    "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "\n",
    "        sse.append(curr_sse)\n",
    "    return sse\n",
    "\n",
    "\n",
    "sse = calculate_WSS(compressed, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:21:35.811064Z",
     "start_time": "2020-04-29T09:21:35.661052Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(sse)\n",
    "plt.ylabel('# SSE')\n",
    "plt.xlabel('# of clusters')\n",
    "plt.title('Choice of # of cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:20:11.504004Z",
     "start_time": "2020-04-29T09:20:11.423157Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters).fit(compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:21:12.740958Z",
     "start_time": "2020-04-29T09:21:12.593124Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.hist(kmeans.labels_)\n",
    "plt.xlabel('Cluster labels')\n",
    "plt.title('histogram of cluster labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:22:30.120345Z",
     "start_time": "2020-04-29T09:22:30.114351Z"
    }
   },
   "source": [
    "Clusters 0 and 5 are retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:34:40.749227Z",
     "start_time": "2020-04-29T09:34:36.504359Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotting\n",
    "t = 0\n",
    "plot_5 = 0\n",
    "cluster_n = 0\n",
    "print('# 5 Examples of cluster 0')\n",
    "for filename in inter:\n",
    "    if len(inter[filename]) and plot_5 < 5:\n",
    "        (fs, data, labels) = loading.loadRecording(filename + '.edf')\n",
    "        folder = '/'.join(filename.split('/')[6:-1])\n",
    "        folder = os.path.join('results-ii', folder)\n",
    "        file = filename.split('/')[-1]\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "        for j in range(len(inter[filename])):\n",
    "            if kmeans.labels_[t] == cluster_n and plot_5 < 5:\n",
    "                a,p = plotting.plot_event(fs, data, labels, inter[filename][j])\n",
    "                plotting.show(p)\n",
    "                plot_5 += 1\n",
    "            t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:35:21.781511Z",
     "start_time": "2020-04-29T09:34:51.997743Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotting\n",
    "t = 0\n",
    "plot_5 = 0\n",
    "cluster_n = 5\n",
    "print('# 5 Examples of cluster 5')\n",
    "for filename in inter:\n",
    "    if len(inter[filename]) and plot_5 < 5:\n",
    "        (fs, data, labels) = loading.loadRecording(filename + '.edf')\n",
    "        folder = '/'.join(filename.split('/')[6:-1])\n",
    "        folder = os.path.join('results-ii', folder)\n",
    "        file = filename.split('/')[-1]\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "        for j in range(len(inter[filename])):\n",
    "            if kmeans.labels_[t] == cluster_n and plot_5 < 5:\n",
    "                a,p = plotting.plot_event(fs, data, labels, inter[filename][j])\n",
    "                plotting.show(p)\n",
    "                plot_5 += 1\n",
    "            t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:47:20.116765Z",
     "start_time": "2020-04-29T10:47:18.469723Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate filters\n",
    "\n",
    "# Average Cov\n",
    "filters = list()\n",
    "for i in [0, 5]:\n",
    "    filters.append(np.mean(rnns[kmeans.labels_ == i,:], axis=0))\n",
    "\n",
    "for i, filt in enumerate(filters):\n",
    "    dim = int(filt.shape[0]**0.5)\n",
    "    w, v = np.linalg.eig(filt.reshape(dim, dim))\n",
    "    index_i = np.argmax(np.cumsum(np.real(w))/np.sum(np.real(w)) > 0.9)\n",
    "    filters[i] = np.real(v[:,:index_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T15:59:19.317811Z",
     "start_time": "2020-04-29T15:59:19.304216Z"
    }
   },
   "outputs": [],
   "source": [
    "from numba import jit, prange\n",
    "\n",
    "def wiener_filter(data, v):\n",
    "    \"\"\"Apply maxSPIR filter.\n",
    "\n",
    "    Args:\n",
    "        data: data contained in an array (row = channels, column = samples)\n",
    "        v: maxSPIR filter as a flattened vector\n",
    "        noise: noise binary mask contained in an array of the same size as data\n",
    "    Return:\n",
    "        out: filtered data\n",
    "    \"\"\"\n",
    "    lag = int(v.shape[0]/data.shape[0])\n",
    "    filtered = list()\n",
    "    for j in prange(v.shape[1]):\n",
    "        v_shaped = np.reshape(v[:,j], (data.shape[0], lag))\n",
    "        out = np.convolve(v_shaped[0, :], data[0, :], 'full')\n",
    "        for i in range(1, v_shaped.shape[0]):\n",
    "            out += np.convolve(v_shaped[i, :], data[i, :], 'full')\n",
    "        filtered.append(out)\n",
    "    t = np.arange(0, v.shape[0], step=lag, dtype=int)\n",
    "    filtered = np.dot(v[t,:], filtered)\n",
    "    return np.array(filtered[:,:data.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:50:33.973823Z",
     "start_time": "2020-04-29T10:50:32.455615Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = '/esat/biomeddata/Neureka_challenge/edf/train/01_tcp_ar/006/00000630/s002_2003_05_28/00000630_s002_t001'\n",
    "\n",
    "import nedc\n",
    "seizures = nedc.loadTSE(filename + '.tse')\n",
    "print('# seizures : {}'.format(len(seizures)))\n",
    "(fs, data, labels) = loading.loadRecording(filename + '.edf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T15:59:29.856923Z",
     "start_time": "2020-04-29T15:59:20.974045Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_0  = wiener_filter(data, filters[0])\n",
    "data_filt0 = data - filtered_0\n",
    "filtered_1  = wiener_filter(data, filters[1])\n",
    "datafiltered_1  = wiener_filter(data_filt0, filters[1])\n",
    "data_filt = data_filt0 - datafiltered_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T15:57:00.359956Z",
     "start_time": "2020-04-29T15:56:59.524504Z"
    }
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "event = inter[filename][j]\n",
    "event = [int(seizures[j][0]*fs) + 4000, int(seizures[j][1]*fs)]\n",
    "\n",
    "a,p = plot_event(fs, data, labels, event)\n",
    "plotting.show(p)\n",
    "a,p = plot_event(fs, data_filt, labels, event)\n",
    "plotting.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T16:03:14.215193Z",
     "start_time": "2020-04-29T16:03:14.189213Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('filters.pickle', 'wb') as handle:\n",
    "    pickle.dump(filters, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
